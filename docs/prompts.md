# ðŸ“ GuÃ­a de Prompts Internos

Esta guÃ­a explica cÃ³mo modificar los prompts internos del sistema para personalizar el comportamiento del anÃ¡lisis con el LLM local (Ollama).

---

## ðŸ“‚ UbicaciÃ³n de los Prompts

Los prompts estÃ¡n definidos en: **`src/orchestration/prompts.py`**

Hay tres bloques principales:

1. **LLM_CONSTITUTION** - Reglas generales de comportamiento
2. **LLM_SPECIFY** - Esquema de salida esperado (JSON)
3. **LLM_PLAN** - Pasos internos del anÃ¡lisis

---

## ðŸ”§ Estructura de los Prompts

### 1. LLM_CONSTITUTION

Define las **reglas inmutables** que el LLM debe seguir:

```python
LLM_CONSTITUTION = """
REGLAS FUNDAMENTALES:
1. No inventes datos. Extrae Ãºnicamente informaciÃ³n presente en el texto.
2. Si una categorÃ­a no aparece, devuÃ©lvela vacÃ­a o con null.
3. Devuelve SIEMPRE un Ãºnico JSON vÃ¡lido que cumpla el esquema exacto.
4. Salida en espaÃ±ol; puedes anotar idioma detectado en "notas" si es Ãºtil.
5. No ofrezcas asesoramiento legal. No aÃ±adas texto fuera del JSON.
"""
```

**CuÃ¡ndo modificar:**
- Agregar reglas especÃ­ficas de tu dominio
- Cambiar idioma de salida por defecto
- Ajustar restricciones de veracidad

**Ejemplo de modificaciÃ³n:**
```python
# AÃ±adir regla para documentos mÃ©dicos
6. Si el documento contiene informaciÃ³n de salud, marca "sensible": true en metadata.
```

### 2. LLM_SPECIFY

Define el **esquema JSON** que el LLM debe generar:

```python
LLM_SPECIFY = """
Dado el contenido textual de un documento, devuelve un JSON con este esquema exacto:
{
  "tipo_documento": "string",
  "partes": ["string"],
  "fechas": [{"etiqueta": "string", "valor": "string"}],
  "importes": [{"concepto": "string", "valor": number|null, "moneda": "string|null"}],
  "obligaciones": ["string"],
  "derechos": ["string"],
  "riesgos": ["string"],
  "resumen_bullets": ["string"],
  "notas": ["string"],
  "confianza_aprox": number
}

Instrucciones:
- Fechas: YYYY-MM-DD cuando sea inequÃ­voco; si no, literal.
- Importes: incluir moneda cuando exista; en otro caso, null.
- Resumen: 5â€“10 bullets, una idea por bullet.
- Riesgos: clÃ¡usulas sensibles (no competencia, penalizaciones, confidencialidad, renuncias).
- Si el texto es escaso/ilegible, reconoce la limitaciÃ³n en "notas".
"""
```

**CuÃ¡ndo modificar:**
- AÃ±adir nuevas categorÃ­as (ej: "ubicaciones", "referencias_legales")
- Cambiar formato de fechas (ISO vs EU)
- Ajustar longitud de resumen (5-10 bullets â†’ 3-7 bullets)

**Ejemplo de modificaciÃ³n:**
```python
# AÃ±adir categorÃ­a de ubicaciones
"ubicaciones": ["string"],  # Ciudades, paÃ­ses mencionados
```

### 3. LLM_PLAN

Define los **pasos internos** que el LLM debe seguir:

```python
LLM_PLAN = """
PASOS INTERNOS:
1. Estimar tipo_documento por patrones (contrato, nÃ³mina, convenio, anexo, desconocido).
2. Extraer PARTES (nombres/razones sociales, Empresa/Empleador/Trabajador/Empleado, CIF/NIF).
3. Detectar FECHAS (inicio, fin, plazos, vencimientos); normalizar a ISO cuando sea claro.
4. Detectar IMPORTES y su contexto (salario, indemnizaciÃ³n, bonus) y moneda.
5. Extraer OBLIGACIONES y DERECHOS a partir de enunciados normativos (frases concisas).
6. SeÃ±alar RIESGOS/ALERTAS (no competencia, confidencialidad, penalizaciones, renuncias).
7. Generar RESUMEN (5â€“10 bullets).
8. Rellenar JSON exactamente como el esquema; nada fuera del JSON.
9. Estimar confianza_aprox por completitud/claridad.
"""
```

**CuÃ¡ndo modificar:**
- Cambiar orden de prioridades
- AÃ±adir heurÃ­sticas especÃ­ficas (ej: "buscar sellos notariales")
- Ajustar criterios de confianza

---

## âš™ï¸ ConfiguraciÃ³n del Modelo Ollama

UbicaciÃ³n: **`config/ollama_config.yaml`**

```yaml
ollama:
  base_url: "http://localhost:11434"
  model: "llama3.2:3b"
  temperature: 0.2        # 0.1-0.5 recomendado
  max_tokens: 4000
  timeout: 120
```

### ParÃ¡metros Clave

#### `model`

Modelos recomendados:

| Modelo | RAM Necesaria | Velocidad | Calidad | Uso Recomendado |
|--------|---------------|-----------|---------|-----------------|
| `llama3.2:3b` | 4GB | RÃ¡pido | Buena | Uso general (recomendado) |
| `phi3:mini` | 2GB | Muy rÃ¡pido | Aceptable | Hardware limitado |
| `mistral:7b` | 8GB | Lento | Excelente | MÃ¡xima calidad |

**Cambiar modelo:**
```bash
# Descargar modelo
ollama pull mistral:7b

# Editar config/ollama_config.yaml
model: "mistral:7b"
```

#### `temperature`

Controla aleatoriedad de la salida:

- **0.1** - Muy determinista, respuestas idÃ©nticas (recomendado para producciÃ³n)
- **0.2** - Balance (default)
- **0.3-0.5** - MÃ¡s variabilidad, Ãºtil para experimentaciÃ³n

**SÃ­ntoma:** JSON invÃ¡lido frecuente â†’ **Bajar temperatura a 0.1**

#### `max_tokens`

Longitud mÃ¡xima de respuesta:

- **2000** - Documentos cortos (1-3 pÃ¡ginas)
- **4000** - Documentos medios (5-10 pÃ¡ginas) - **Default**
- **8000** - Documentos largos (chunking automÃ¡tico se activa)

---

## ðŸ”„ Reintentos AutomÃ¡ticos

UbicaciÃ³n: **`src/orchestration/analyzer.py`**

```python
MAX_RETRIES = 2  # NÃºmero de reintentos si JSON invÃ¡lido
```

Si el LLM devuelve JSON invÃ¡lido:

1. **Primer intento:** EnvÃ­a prompt original
2. **Segundo intento:** EnvÃ­a prompt con correcciÃ³n: _"Tu respuesta anterior no era JSON vÃ¡lido. Devuelve SOLO JSON con el esquema..."_
3. **Tercer intento:** Ãšltimo intento con mayor Ã©nfasis

Si tras 3 intentos falla â†’ ExcepciÃ³n `ValidationError`

---

## ðŸ“Š Ejemplos de PersonalizaciÃ³n

### Ejemplo 1: Documentos MÃ©dicos

```python
# prompts.py
LLM_SPECIFY = """
{
  // ... campos existentes ...
  "diagnosticos": ["string"],
  "medicamentos": [{"nombre": "string", "dosis": "string"}],
  "alergias": ["string"]
}
"""

LLM_PLAN = """
// ... pasos existentes ...
10. Extraer DIAGNÃ“STICOS del informe mÃ©dico
11. Listar MEDICAMENTOS con dosis prescritas
12. Identificar ALERGIAS o contraindicaciones
"""
```

### Ejemplo 2: Contratos en InglÃ©s

```python
# prompts.py
LLM_CONSTITUTION = """
1. Extract information ONLY from the document text.
2. Return answers in ENGLISH.
3. Use ISO date format: YYYY-MM-DD.
"""
```

```yaml
# config/ollama_config.yaml
model: "llama3.2:3b"
language: "en"  # AÃ±adir campo personalizado
```

### Ejemplo 3: Resumen MÃ¡s Largo

```python
# prompts.py
LLM_SPECIFY = """
"resumen_bullets": ["string"],  # 10-15 bullets (aumentado desde 5-10)
"""

LLM_PLAN = """
7. Generar RESUMEN (10â€“15 bullets, mÃ¡ximo detalle).
"""
```

---

## ðŸ§ª Testing de Prompts

Para probar cambios en prompts:

1. **Edita** `src/orchestration/prompts.py`
2. **Reinicia** la aplicaciÃ³n: `streamlit run src/ui/app.py`
3. **Analiza** documento de prueba
4. **Revisa** salida en pantalla y JSON exportado

**Script de prueba directa:**
```bash
python -c "from src.orchestration.ollama_client import OllamaClient; client = OllamaClient(); print(client.generate('Tu prompt aquÃ­'))"
```

---

## âš ï¸ Advertencias

1. **No eliminar campos requeridos** del esquema JSON - romperÃ¡ validaciÃ³n Pydantic
2. **Temperatura > 0.5** puede causar JSON invÃ¡lido frecuente
3. **Prompts muy largos** (> 2000 palabras) aumentan latencia sin mejorar resultados
4. **Reiniciar aplicaciÃ³n** tras cambios en prompts (no se recargan dinÃ¡micamente)

---

## ðŸ“š Recursos Adicionales

- **Ollama Model Library:** https://ollama.ai/library
- **Prompt Engineering Guide:** https://www.promptingguide.ai/
- **Pydantic Docs:** https://docs.pydantic.dev/

---

**Â¿Necesitas ayuda?** Consulta `docs/troubleshooting.md` si los cambios no funcionan como esperas.
