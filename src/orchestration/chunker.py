"""
Document Chunker and Analysis Consolidator - Analizador de Documentos Legales

Divide documentos largos en chunks manejables para el LLM y consolida
los anÃ¡lisis resultantes en uno solo, con deduplicaciÃ³n y reconciliaciÃ³n.

Author: Analizador de Documentos Legales Team
Date: 2026-02-18
"""

import re
from typing import List, Dict, Any
from collections import Counter

from src.models.analisis import Analisis, Fecha, Importe


def split_text(
    texto: str,
    max_chunk_size: int = 12000,
    overlap: int = 200
) -> List[str]:
    """
    Divide texto largo en chunks balanceados sin cortar oraciones

    Args:
        texto: Texto completo a dividir
        max_chunk_size: TamaÃ±o mÃ¡ximo de cada chunk en caracteres
        overlap: Solapamiento entre chunks para preservar contexto

    Returns:
        List[str]: Lista de chunks de texto

    Example:
        >>> texto_largo = "..." * 5000
        >>> chunks = split_text(texto_largo, max_chunk_size=10000)
        >>> len(chunks)
        3
        >>> all(len(c) <= 10000 for c in chunks)
        True
    """
    if len(texto) <= max_chunk_size:
        return [texto]

    chunks = []
    start = 0

    while start < len(texto):
        # Calcular el final del chunk
        end = start + max_chunk_size

        if end >= len(texto):
            # Ãšltimo chunk
            chunks.append(texto[start:].strip())
            break

        # Buscar el final de una oraciÃ³n cerca del lÃ­mite
        # Intentar cortar en: punto + espacio, punto + salto de lÃ­nea
        search_start = max(start + max_chunk_size - 500, start)
        search_end = min(end + 200, len(texto))
        search_text = texto[search_start:search_end]

        # Patrones de fin de oraciÃ³n
        sentence_ends = list(re.finditer(r'[.!?]\s+', search_text))

        if sentence_ends:
            # Usar el Ãºltimo fin de oraciÃ³n encontrado
            last_match = sentence_ends[-1]
            actual_end = search_start + last_match.end()
        else:
            # Si no hay fin de oraciÃ³n, buscar al menos un espacio
            last_space = texto[:end].rfind(' ')
            if last_space > start + max_chunk_size * 0.8:
                actual_end = last_space
            else:
                actual_end = end

        # AÃ±adir chunk
        chunks.append(texto[start:actual_end].strip())

        # Avanzar con overlap
        start = actual_end - overlap

    return chunks


def deduplicate_list(items: List[str], similarity_threshold: float = 0.85) -> List[str]:
    """
    Elimina duplicados de una lista de strings, incluyendo similares

    Args:
        items: Lista de strings
        similarity_threshold: Umbral de similitud para considerar duplicados

    Returns:
        List[str]: Lista sin duplicados

    Example:
        >>> items = ["Pago de 1000 EUR", "Pago de 1000 EUR", "BonificaciÃ³n"]
        >>> deduplicate_list(items)
        ['Pago de 1000 EUR', 'BonificaciÃ³n']
    """
    if not items:
        return []

    # Exactos primero
    seen = set()
    unique = []

    for item in items:
        item_lower = item.lower().strip()
        if item_lower not in seen:
            seen.add(item_lower)
            unique.append(item)

    # Similitud simple: si A contiene >80% de B, son duplicados
    deduplicated = []
    for i, item_a in enumerate(unique):
        is_duplicate = False
        for j, item_b in enumerate(unique):
            if i == j:
                continue

            # Si item_a estÃ¡ mayormente contenido en item_b
            words_a = set(item_a.lower().split())
            words_b = set(item_b.lower().split())

            if not words_a:
                continue

            overlap = len(words_a & words_b) / len(words_a)

            if overlap >= similarity_threshold and len(item_b) > len(item_a):
                # item_a es redundante respecto a item_b
                is_duplicate = True
                break

        if not is_duplicate:
            deduplicated.append(item_a)

    return deduplicated


def merge_fechas(fechas_lists: List[List[Fecha]]) -> List[Fecha]:
    """
    Fusiona listas de fechas, eliminando duplicados

    Args:
        fechas_lists: Lista de listas de Fecha

    Returns:
        List[Fecha]: Lista consolidada sin duplicados
    """
    all_fechas = [f for sublist in fechas_lists for f in sublist]

    if not all_fechas:
        return []

    # Deduplicar por (etiqueta, valor)
    seen_keys = set()
    unique_fechas = []

    for fecha in all_fechas:
        key = (fecha.etiqueta.lower().strip(), fecha.valor.lower().strip())
        if key not in seen_keys:
            seen_keys.add(key)
            unique_fechas.append(fecha)

    return unique_fechas


def merge_importes(importes_lists: List[List[Importe]]) -> List[Importe]:
    """
    Fusiona listas de importes, eliminando duplicados y reconciliando conflictos

    Args:
        importes_lists: Lista de listas de Importe

    Returns:
        List[Importe]: Lista consolidada
    """
    all_importes = [imp for sublist in importes_lists for imp in sublist]

    if not all_importes:
        return []

    # Agrupar por concepto similar
    grouped: Dict[str, List[Importe]] = {}

    for importe in all_importes:
        concepto_norm = importe.concepto.lower().strip()

        # Buscar si ya existe un grupo similar
        found_key = None
        for key in grouped.keys():
            # Similitud simple: >60% de palabras en comÃºn
            words_key = set(key.split())
            words_concepto = set(concepto_norm.split())

            if not words_key or not words_concepto:
                continue

            overlap = len(words_key & words_concepto) / max(len(words_key), len(words_concepto))

            if overlap > 0.6:
                found_key = key
                break

        if found_key:
            grouped[found_key].append(importe)
        else:
            grouped[concepto_norm] = [importe]

    # Consolidar cada grupo
    unique_importes = []

    for concepto_key, imps in grouped.items():
        # Si todos tienen el mismo valor y moneda, tomar el primero
        valores = [imp.valor for imp in imps if imp.valor is not None]
        monedas = [imp.moneda for imp in imps if imp.moneda is not None]

        if len(set(valores)) == 1 and len(set(monedas)) <= 1:
            # Valores consistentes
            unique_importes.append(imps[0])
        else:
            # Conflicto: usar el concepto mÃ¡s comÃºn y anotar variabilidad
            conceptos_originales = [imp.concepto for imp in imps]
            concepto_final = Counter(conceptos_originales).most_common(1)[0][0]

            # Valor mÃ¡s frecuente
            if valores:
                valor_final = Counter(valores).most_common(1)[0][0]
            else:
                valor_final = None

            # Moneda mÃ¡s frecuente
            if monedas:
                moneda_final = Counter(monedas).most_common(1)[0][0]
            else:
                moneda_final = None

            unique_importes.append(Importe(
                concepto=concepto_final,
                valor=valor_final,
                moneda=moneda_final
            ))

    return unique_importes


def consolidate_analyses(chunks_analisis: List[Analisis]) -> Analisis:
    """
    Consolida mÃºltiples anÃ¡lisis de chunks en uno solo

    Estrategia:
    - tipo_documento: votaciÃ³n por mayorÃ­a
    - listas (partes, obligaciones, etc.): merge + deduplicaciÃ³n
    - resumen_bullets: top 10 mÃ¡s frecuentes
    - confianza_aprox: promedio ponderado por completitud

    Args:
        chunks_analisis: Lista de anÃ¡lisis de cada chunk

    Returns:
        Analisis: AnÃ¡lisis consolidado

    Example:
        >>> a1 = Analisis(tipo_documento="contrato", partes=["ACME"], confianza_aprox=0.8)
        >>> a2 = Analisis(tipo_documento="contrato", partes=["ACME", "Juan"], confianza_aprox=0.9)
        >>> consolidado = consolidate_analyses([a1, a2])
        >>> consolidado.tipo_documento
        'contrato'
        >>> len(consolidado.partes)
        2
    """
    if not chunks_analisis:
        raise ValueError("No analyses to consolidate")

    if len(chunks_analisis) == 1:
        return chunks_analisis[0]

    # 1. Tipo de documento: votaciÃ³n
    tipos = [a.tipo_documento for a in chunks_analisis]
    tipo_final = Counter(tipos).most_common(1)[0][0]

    # 2. Partes: merge + deduplicaciÃ³n
    partes_all = [a.partes for a in chunks_analisis]
    partes_merged = [p for sublist in partes_all for p in sublist]
    partes_final = deduplicate_list(partes_merged)

    # 3. Fechas: merge sin duplicados
    fechas_all = [a.fechas for a in chunks_analisis]
    fechas_final = merge_fechas(fechas_all)

    # 4. Importes: merge + reconciliaciÃ³n
    importes_all = [a.importes for a in chunks_analisis]
    importes_final = merge_importes(importes_all)

    # 5. Obligaciones: merge + deduplicaciÃ³n
    obligaciones_all = [a.obligaciones for a in chunks_analisis]
    obligaciones_merged = [o for sublist in obligaciones_all for o in sublist]
    obligaciones_final = deduplicate_list(obligaciones_merged)

    # 6. Derechos: merge + deduplicaciÃ³n
    derechos_all = [a.derechos for a in chunks_analisis]
    derechos_merged = [d for sublist in derechos_all for d in sublist]
    derechos_final = deduplicate_list(derechos_merged)

    # 7. Riesgos: merge + deduplicaciÃ³n
    riesgos_all = [a.riesgos for a in chunks_analisis]
    riesgos_merged = [r for sublist in riesgos_all for r in sublist]
    riesgos_final = deduplicate_list(riesgos_merged)

    # 8. Resumen bullets: top 10 mÃ¡s frecuentes
    bullets_all = [b for a in chunks_analisis for b in a.resumen_bullets]
    bullet_counts = Counter(bullets_all)

    # Si hay duplicados exactos, usar los mÃ¡s comunes; si no, tomar los primeros 10
    if len(bullet_counts) > 10:
        resumen_final = [b for b, _ in bullet_counts.most_common(10)]
    else:
        resumen_final = deduplicate_list(bullets_all)[:10]

    # 9. Notas: merge todas + aÃ±adir nota de consolidaciÃ³n
    notas_all = [n for a in chunks_analisis for n in a.notas]
    notas_final = deduplicate_list(notas_all)
    notas_final.insert(0, f"AnÃ¡lisis consolidado de {len(chunks_analisis)} fragmentos del documento")

    # 10. Confianza: promedio ponderado
    # Peso = nÃºmero de categorÃ­as con datos
    confianzas = []
    pesos = []

    for analisis in chunks_analisis:
        peso = sum([
            bool(analisis.partes),
            bool(analisis.fechas),
            bool(analisis.importes),
            bool(analisis.obligaciones),
            bool(analisis.derechos),
            bool(analisis.riesgos),
            bool(analisis.resumen_bullets)
        ])
        confianzas.append(analisis.confianza_aprox)
        pesos.append(max(peso, 1))  # Al menos peso 1

    confianza_final = sum(c * p for c, p in zip(confianzas, pesos)) / sum(pesos)
    confianza_final = round(confianza_final, 2)

    # Construir anÃ¡lisis consolidado
    return Analisis(
        tipo_documento=tipo_final,
        partes=partes_final,
        fechas=fechas_final,
        importes=importes_final,
        obligaciones=obligaciones_final,
        derechos=derechos_final,
        riesgos=riesgos_final,
        resumen_bullets=resumen_final,
        notas=notas_final,
        confianza_aprox=confianza_final
    )


if __name__ == "__main__":
    # Test de chunker
    print("=" * 60)
    print("Testing Document Chunker and Consolidator")
    print("=" * 60)

    # Test 1: Split largo
    print("\nðŸ“‹ Test 1: Text splitting")
    texto_largo = ("Este es un texto muy largo. " * 200)  # ~5400 chars
    chunks = split_text(texto_largo, max_chunk_size=2000, overlap=100)
    print(f"âœ… Original: {len(texto_largo)} chars")
    print(f"âœ… Chunks: {len(chunks)}")
    print(f"âœ… Chunk sizes: {[len(c) for c in chunks]}")
    print(f"âœ… Max chunk: {max(len(c) for c in chunks)} chars")

    # Test 2: DeduplicaciÃ³n
    print("\nðŸ“‹ Test 2: Deduplication")
    items = [
        "Pago mensual de 1000 EUR",
        "Pago mensual de 1000 EUR",
        "BonificaciÃ³n anual de 2000 EUR",
        "pago mensual de 1000 eur",  # Case insensitive
        "Vacaciones de 30 dÃ­as"
    ]
    deduplicated = deduplicate_list(items)
    print(f"âœ… Original: {len(items)} items")
    print(f"âœ… Deduplicated: {len(deduplicated)} items")
    for item in deduplicated:
        print(f"   - {item}")

    # Test 3: ConsolidaciÃ³n de anÃ¡lisis
    print("\nðŸ“‹ Test 3: Analysis consolidation")

    from src.models.analisis import Fecha, Importe

    analisis_1 = Analisis(
        tipo_documento="contrato_laboral",
        partes=["ACME Corp", "Juan PÃ©rez"],
        fechas=[Fecha(etiqueta="Inicio", valor="2026-03-01")],
        importes=[Importe(concepto="Salario base", valor=30000.0, moneda="EUR")],
        obligaciones=["No competir durante 2 aÃ±os"],
        derechos=["30 dÃ­as de vacaciones"],
        riesgos=["ClÃ¡usula de no competencia"],
        resumen_bullets=["Contrato anual", "Salario 30k EUR"],
        confianza_aprox=0.85
    )

    analisis_2 = Analisis(
        tipo_documento="contrato_laboral",
        partes=["ACME Corp", "Juan PÃ©rez", "Supervisor: MarÃ­a GarcÃ­a"],
        fechas=[
            Fecha(etiqueta="Inicio", valor="2026-03-01"),
            Fecha(etiqueta="RenovaciÃ³n", valor="2027-03-01")
        ],
        importes=[
            Importe(concepto="Salario base", valor=30000.0, moneda="EUR"),
            Importe(concepto="Bonus anual", valor=5000.0, moneda="EUR")
        ],
        obligaciones=["No competir durante 2 aÃ±os", "Confidencialidad perpetua"],
        derechos=["30 dÃ­as de vacaciones", "Seguro mÃ©dico privado"],
        riesgos=["ClÃ¡usula de no competencia", "PenalizaciÃ³n por incumplimiento"],
        resumen_bullets=["Contrato anual", "Salario 30k EUR", "Bonus 5k EUR"],
        confianza_aprox=0.90
    )

    consolidado = consolidate_analyses([analisis_1, analisis_2])

    print(f"âœ… Tipo: {consolidado.tipo_documento}")
    print(f"âœ… Partes: {len(consolidado.partes)} â†’ {consolidado.partes}")
    print(f"âœ… Fechas: {len(consolidado.fechas)}")
    print(f"âœ… Importes: {len(consolidado.importes)}")
    print(f"âœ… Obligaciones: {len(consolidado.obligaciones)}")
    print(f"âœ… Derechos: {len(consolidado.derechos)}")
    print(f"âœ… Riesgos: {len(consolidado.riesgos)}")
    print(f"âœ… Resumen: {len(consolidado.resumen_bullets)} bullets")
    print(f"âœ… Confianza: {consolidado.confianza_aprox}")
    print(f"âœ… Notas: {consolidado.notas[0]}")

    print("\nâœ… Chunker ready!")
